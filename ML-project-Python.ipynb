{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Implemented on Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Students: Minh Hai NGUYEN - Cam Thanh Ha LE - Ayoub EL ARROUD EL HADARI - Paul Corbalan\n",
    "    Supervisor: Mme. BÃ©atrice LAURENT-BONNEAU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Data Loading and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load some useful libraries for data loading and data visualization in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from math import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_DataSet(name, data_location = \"data/\"):\n",
    "    return pd.read_csv(data_location + name + \".txt\", sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "path = \"\"\n",
    "data_location = \"data/\"\n",
    "rain = Load_DataSet(\"rain_project\", data_location = path + data_location)\n",
    "# Let's take a look at the data\n",
    "rain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**:\n",
    "\n",
    "In this database, we realise that the qualitative variables including \"Id\", \"date\", \"rain_class\". \n",
    "\n",
    "The other variables are considered quantitative including \"ff\",\"t\", \"td\", \"hu\", \"dd\", \"precip\", \"ws_arome\", \"p3031_arome\", \"u10_arome\", \"v10_arome\", \"t2m_arome\", \"d2m_arome\", \"r_arome\", \"tp_arome\", \"msl_arome\", \"rain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the column \"date\" into \"month\" to obtain the new data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "rain[\"date\"] = pd.to_datetime(rain[\"date\"]).dt.month\n",
    "rain = rain.rename(columns= {\"date\":\"month\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(rain.columns)\n",
    "num_var = names[2:-1]\n",
    "qual_var = [names[i] for i in [0,1,-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithm transformation ($\\log(\\cdot + 1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_log = rain.copy()\n",
    "\n",
    "rain_log[\"precip\"] = np.log(rain_log[\"precip\"] + 1)\n",
    "rain_log[\"tp_arome\"] = np.log(rain_log[\"tp_arome\"] + 1)\n",
    "rain_log[\"rain_log\"] = np.log(rain_log[\"rain\"] + 1)\n",
    "\n",
    "rain_log.rename(columns = {'precip':'precip_log', 'tp_arome':'tp_arome_log'}, inplace = True)\n",
    "\n",
    "num_var_log = num_var\n",
    "num_var_log = list(map(lambda item: item.replace(\"precip\",\"precip_log\"), num_var_log))\n",
    "num_var_log = list(map(lambda item: item.replace(\"tp_arome\",\"tp_arome_log\"), num_var_log))\n",
    "\n",
    "qual_var_log = qual_var + [\"rain_log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Rain data set presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description of the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rain.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rain_log.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative variable\n",
    "#### Histogram of `month` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"month\"\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(rain[var], bins = 2*12-1)\n",
    "plt.title(\"Histogram of \"+var)\n",
    "plt.xlabel(var+\" values\")\n",
    "plt.ylabel(\"Number per interval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitatives variables\n",
    "#### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the name of variables in `var_names` variables and quantitative variables, qualitatives variables as `num_var` and `qual_var` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(rain.columns)\n",
    "num_var = names[2:-1]\n",
    "qual_var = [names[i] for i in [0,1,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig, axs = plt.subplots(4,4,figsize=(16,16))\n",
    "axs = axs.ravel()\n",
    "for i in range(16):\n",
    "    # sns.histplot(rain[num_var[i]], ax = axs[i])\n",
    "    sns.histplot(data=rain[num_var[i]], stat='density', bins=40, kde=True, color=\"skyblue\", ax=axs[i])\n",
    "    sns.kdeplot(rain[num_var[i]], color='red', ax=axs[i])\n",
    "    axs[i].set_xlabel(\"\")\n",
    "    axs[i].set_title(num_var[i])\n",
    "plt.tight_layout()\n",
    "# sns.histplot(rain['ff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between variables\n",
    "##### Classical data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(rain[num_var], figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_corr = rain[num_var].corr()\n",
    "\n",
    "mask = np.zeros_like(rain_corr, dtype=np.bool_)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(12, 14))\n",
    "cmap= 'coolwarm'\n",
    "sns.heatmap(rain_corr, mask=mask, cmap=cmap, annot=True, vmax=.3, vmin=-.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logarithmical data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(rain_log[num_var_log], figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_log_corr = rain_log[num_var_log].corr()\n",
    "\n",
    "mask = np.zeros_like(rain_log_corr, dtype=np.bool_)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(12, 14))\n",
    "cmap= 'coolwarm'\n",
    "sns.heatmap(rain_log_corr, mask=mask, cmap=cmap, annot=True, vmax=.3, vmin=-.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.4. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var_in = num_var[:-1]\n",
    "\n",
    "pcaR = PCA()\n",
    "loadingR = pd.DataFrame(scale(rain[num_var_in]), columns = rain[num_var_in].columns)\n",
    "pca_DataSet = pcaR.fit(loadingR).transform(loadingR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "x = np.arange(pcaR.explained_variance_ratio_.size)\n",
    "plt.bar(x, pcaR.explained_variance_ratio_*100)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "x = np.arange(pcaR.explained_variance_ratio_.size)\n",
    "plt.bar(x, pcaR.explained_variance_ratio_.cumsum()*100)\n",
    "plt.plot(x, np.zeros(x.shape)+95, color  =\"red\")\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative summation of explained variance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_PCA_components = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_DataSet = pd.DataFrame(pca_DataSet)\n",
    "pca_DataSet[\"rain_class\"] = rain[\"rain_class\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_DataSet.iloc[:,0:nb_PCA_components].plot(kind = \"box\", figsize = (15, 6) )\n",
    "plt.xlabel('First %d-th principal components' % nb_PCA_components)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(5):\n",
    "    pca_DataSet.plot.scatter(x=dim, y=dim+1, c=\"rain_class\", cmap=\"viridis\", figsize = (10, 10))\n",
    "    plt.xlabel(\"Dim \"+str(dim+1))\n",
    "    plt.ylabel(\"Dim \"+str(dim+2))\n",
    "    plt.title('Individuals factor map - PCA')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(5):\n",
    "    coord1 = pcaR.components_[dim] * np.sqrt(pcaR.explained_variance_[dim])\n",
    "    coord2 = pcaR.components_[dim+1] * np.sqrt(pcaR.explained_variance_[dim+1])\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    for i, j, nom in zip(coord1, coord2, loadingR.columns):\n",
    "        plt.text(i, j, nom)\n",
    "        plt.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "    plt.axis((-1, 1, -1, 1))\n",
    "    plt.xlabel(\"Dim \"+str(dim+1))\n",
    "    plt.ylabel(\"Dim \"+str(dim+2))\n",
    "    #Cercle\n",
    "    c = plt.Circle((0, 0), radius = 1, color = 'b', fill = False)\n",
    "    ax.add_patch(c)\n",
    "    plt.title('Variables factor map - PCA')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into a training set and a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple comparison purpose, we decide to use the same extraction in both Python and R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/train_set.txt\",sep = ' ')\n",
    "test_set = pd.read_csv(\"data/test_set.txt\",sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_set).copy()\n",
    "del X_train['rain']\n",
    "del X_train['rain_class']\n",
    "del X_train['rain_log']\n",
    "\n",
    "X_test = pd.DataFrame(test_set).copy()\n",
    "del X_test['rain']\n",
    "del X_test['rain_class']\n",
    "del X_test['rain_log']\n",
    "train_set['rain_class'] = train_set['rain_class'].astype(\"category\")\n",
    "test_set['rain_class'] = test_set['rain_class'].astype(\"category\")\n",
    "Y_train = train_set['rain']\n",
    "Y_train_log = train_set['rain_log']\n",
    "\n",
    "Y_test = test_set['rain']\n",
    "Y_test_log = test_set['rain_log']\n",
    "Y_train_class = train_set['rain_class']\n",
    "Y_test_class = test_set['rain_class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metric(y_true, y_pred):\n",
    "    # labs = [\"high_rain\", \"low_rain\", \"no_rain\"]\n",
    "    # f1 = np.round(f1_score(y_true, y_pred, labels = labs, average = \"macro\"), 3)\n",
    "    # class_recall = np.round(recall_score(y_true, y_pred, labels = labs, average = None), 3)\n",
    "    # total_recall = np.round(recall_score(y_true, y_pred, labels = labs, average = \"macro\"), 3)\n",
    "    # acc = np.round(accuracy_score(y_true, y_pred), 3)\n",
    "    # precision = np.round(precision_score(y_true, y_pred, labels = labs, average = None), 3)\n",
    "    print(\"Confusion matrix : \")\n",
    "    table = pd.crosstab(y_true, y_pred, dropna = False)\n",
    "    print(table)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    # table.columns = table.columns.astype(str)\n",
    "    # table[\"recall\"] = class_recall[:len(table.index)]\n",
    "    # table[\"precision\"] = precision[:len(table.index)]\n",
    "    # print(\"The confusion matrix \")\n",
    "    # print(table)\n",
    "    # print(\"The prediction accuracy: \", acc)\n",
    "    # print(\"The f1-score : \", f1)\n",
    "    # print(\"The recall-score : \", total_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. K nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The completeness parameter `k` is optimised on a predefined grid by minimising the estimated error by cross-validation; scikit-learn offers many cross-validation options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Optimisation of k\n",
    "param_grid = [{\"n_neighbors\": list(range(1, 20))}]\n",
    "knn = GridSearchCV(KNeighborsClassifier(weights = \"distance\"), scoring = \"accuracy\", param_grid = param_grid, cv=10, n_jobs=-1, refit = True)\n",
    "knnOpt = knn.fit(X_train, Y_train_class)  \n",
    "# optimal parameter\n",
    "# knnOpt.best_params_[\"n_neighbors\"]\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (knnOpt.best_score_, knnOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction accuracy in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the prediction accuracy on the test sample\n",
    "print(\"Prediction accuracy in the test sample : \", knnOpt.score(X_test, Y_test_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_hat_class = knnOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_hat_class)\n",
    "# print(\"Accuracy score =\", accuracy_score(y_true = Y_test_class, y_pred = y_hat_class))\n",
    "\n",
    "# # confusion matrix\n",
    "# table = pd.crosstab(y_hat_class, Y_test_class)\n",
    "# print(\"Confusion matrix\")\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Optimisation of shaft depth\n",
    "param_grid = [{\"max_depth\": range(2,10), \"min_samples_split\" : range(2,10), \"min_samples_leaf\": range(1,5) }]\n",
    "tree = GridSearchCV(DecisionTreeClassifier(max_features = 'auto', min_impurity_decrease = 1e-3, random_state = 42), scoring = \"accuracy\", param_grid = param_grid, cv=10, n_jobs=-1, refit = True)\n",
    "treeOpt = tree.fit(X_train, Y_train_class)\n",
    "# Optimal parameter\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (treeOpt.best_score_, treeOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the prediction error on the test sample\n",
    "treeOpt.score(X_test, Y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_hat_class = treeOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_hat_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The `low_rain` class seems to be difficult to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# Parameters' definitions\n",
    "forest = RandomForestClassifier(n_estimators = 500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# Training\n",
    "rfFit = forest.fit(X_train,Y_train_class)\n",
    "# Out-of-bag error on the train sample\n",
    "print(1 - rfFit.oob_score_)\n",
    "# Out-of-bag error on the test sample\n",
    "print(1 - rfFit.score(X_test,Y_test_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters tunning by cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\"n_estimators\" : range(200, 500, 100) , \"max_features\": range(2,5), \"max_depth\" : range(10, 20, 2) }]\n",
    "rf = GridSearchCV(RandomForestClassifier(random_state=42, criterion=\"entropy\"),\n",
    "        param, cv = 5, n_jobs=-1)\n",
    "rfOpt = rf.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (rfOpt.best_score_, rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test = treeOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting applying to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = XGBRFClassifier(n_estimators = 500, subsample = 0.99, random_state = 42, metric = \"accuracy\", n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X_train, Y_train_class, scoring='accuracy', cv = cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train_class)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.1. Linear SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation of C - Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"C\": np.linspace(0.01, 0.1, 10) }]\n",
    "svm= GridSearchCV(LinearSVC(), param, cv=10, n_jobs = -1, scoring = \"accuracy\")\n",
    "svmLinOpt=svm.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svmLinOpt.best_score_,svmLinOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Y_test_class == 'no_rain').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test = svmLinOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    It seems that with the linear kernel, the results is quite good comparing to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.2. SVM with polynomial kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we take polynomial of degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param = [{\"C\":np.linspace(0.75,1.25,10),\"gamma\":np.linspace(0.01, 0.1, 10) , \"coef0\": np.linspace(0, 3, 5) }]\n",
    "svm = GridSearchCV(SVC(kernel=\"poly\"),param,cv=10,n_jobs=-1, scoring = \"accuracy\")\n",
    "svmPolyOpt=svm.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svmPolyOpt.best_score_, svmPolyOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test = svmPolyOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"C\" : np.linspace(0.75,1.25,10),\"gamma\":np.linspace(0.01, 0.1, 10), \"coef0\":np.linspace(0, 3, 5) }]\n",
    "svm= GridSearchCV(SVC(kernel=\"poly\",degree =2),param,cv=10,n_jobs=-1,scoring = \"accuracy\")\n",
    "svmPoly2Opt=svm.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svmPoly2Opt.best_score_,svmPoly2Opt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test= svmPoly2Opt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.3. SVM with radial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"C\" : np.linspace(1,1.25,10),\"gamma\":np.linspace(0.05, 0.15, 10)}]\n",
    "svm= GridSearchCV(SVC(kernel=\"rbf\"), param, cv=10, n_jobs=-1,scoring = \"accuracy\")\n",
    "svmRadOpt=svm.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svmRadOpt.best_score_,svmRadOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test = svmRadOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.3. SVM with sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"C\":[0.1,0.4,0.5,0.6,0.8,1,1.2,1.4,1.6,2],\"gamma\":np.array(range(1,11))/100, \"coef0\":np.array(range(1,11))/10}]\n",
    "svm= GridSearchCV(SVC(kernel=\"sigmoid\"),param,cv=10,n_jobs=-1,scoring = \"accuracy\")\n",
    "svmSigOpt=svm.fit(X_train, Y_train_class)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svmSigOpt.best_score_,svmSigOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test sample\n",
    "y_pred_test = svmSigOpt.predict(X_test)\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.5. Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5.1. Multi-layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hidden layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we fit an one-hidden layer neural network with ReLU activation in the hidden layer and the softmax activation for the out put layer, and the log-loss function. We use here the default configuration for the optimisation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1 = MLPClassifier(hidden_layer_sizes = (3), random_state = 42, max_iter = 1500)\n",
    "nnet1.fit(X_train, Y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nnet1.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nnet1.predict(X_train)\n",
    "\n",
    "classification_metric(Y_train_class, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = nnet1.predict(X_test)\n",
    "\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Since the training accuracy is low, we can not expect that the test accuracy is good. It means that this model does fit with the data or the optimisation process did not minimize the loss function since it stopped around 500 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross validation for searching the optimal number of neurones in the hidden layer and the learning rate for optimisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"hidden_layer_sizes\" : [(3,),(4,),(5,),(6,), (7,)], \"early_stopping\" : [True, False], \"alpha\" : [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]}]\n",
    "\n",
    "nnet1_cv = GridSearchCV(MLPClassifier(max_iter = 1500, random_state = 42), param_grid, cv = 5, n_jobs=-1, return_train_score = True)\n",
    "nnet1_cv.fit(X_train, Y_train_class)\n",
    "\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (nnet1_cv.best_score_, nnet1_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nnet1_cv.predict(X_train)\n",
    "\n",
    "classification_metric(Y_train_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = nnet1_cv.predict(X_test)\n",
    "\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss function is almost reached the minimum but the prediction accuracy is still arounded $50$ percents. It means that this model does not fit well the data. We will try with more sophisticated models in the next sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayers Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Remark that if we just fit a sophisticated model with many parameters (weights and biases), we can get very high performance on the training set. But unfortunately, we can get a worse performance on the test set\n",
    "    and the generalization of the model is very bad. This is the **overfitting** phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_of = MLPClassifier(hidden_layer_sizes = (20, 25 , 15, 15, 10, 5), random_state = 42, max_iter = 1500, alpha = 0.1, activation = \"relu\", early_stopping = False, n_iter_no_change = 75)\n",
    "nnet_of.fit(X_train, Y_train_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy : \", nnet_of.score(X_train, Y_train_class))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(nnet_of.loss_curve_)\n",
    "plt.title(\"Loss curve during training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_nn_overfit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = nnet_of.predict(X_test)\n",
    "\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try here to find a Neural Network which can generalize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLPClassifier(hidden_layer_sizes = (15, 10, 10, 8, 8, 5), random_state = 42, max_iter = 1500, alpha = 0.1, activation = \"tanh\", early_stopping = True, n_iter_no_change = 500)\n",
    "nnet.fit(X_train, Y_train_class)\n",
    "\n",
    "print(\"Training accuracy : \", nnet.score(X_train, Y_train_class))\n",
    "plt.plot(nnet.loss_curve_)\n",
    "plt.title(\"Loss curve during training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()\n",
    "y_pred_test = nnet.predict(X_test)\n",
    "\n",
    "classification_metric(Y_test_class, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15, 15, 10, 5: 0.529"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15, 15, 8, 8, 5, tanh : 0.536\n",
    "\n",
    "15, 10, 10, 8, 8, 5, tanh: 0.565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.6. Gaussian Process Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import Matern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPC = GaussianProcessClassifier(kernel = 0.5*RBF(3.0), random_state = 42, multi_class = \"one_vs_rest\")\n",
    "GPC.fit(X_train, Y_train_class)\n",
    "print(GPC.score(X_train, Y_train_class))\n",
    "print(GPC.score(X_test, Y_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPC = GaussianProcessClassifier(kernel = Matern(3.0, nu = 1.5), random_state = 42, multi_class = \"one_vs_rest\")\n",
    "GPC.fit(X_train, Y_train_class)\n",
    "print(GPC.score(X_train, Y_train_class))\n",
    "print(GPC.score(X_test, Y_test_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.REGRESSION AND REGRESSION FOR CLASSIFICATION USING THRESHOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used in regression problem: MSE, MAPE et R2-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metric(y_true, y_pred):\n",
    "    mape = MAPE(y_true, y_pred)\n",
    "    mse = MSE(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = MAE(y_true, y_pred)\n",
    "\n",
    "    print(\"Mean squared error : \", np.round(mse, 2))\n",
    "    print(\"Mean absolute error : \", np.round(mae, 2))\n",
    "    print(\"Mean absolute percentage error : \", np.round(mape, 2))\n",
    "    print(\"R2 - Score : \", np.round(r2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(rain, eps = 1e-3):\n",
    "    myrain = (rain <= eps)*0 + (rain > eps)*(rain <= 2 )*1 + (rain > 2)*2\n",
    "    myrain2 = myrain.astype('<U9')\n",
    "\n",
    "    myrain2[myrain == 0] = 'no_rain'\n",
    "    myrain2[myrain == 1] = 'low_rain'\n",
    "    myrain2[myrain == 2] = 'high_rain'\n",
    "    return myrain2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Linear regression without penalisation and without variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.1. With `rain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regLin = LinearRegression().fit(X_train, Y_train)\n",
    "y_pred = regLin.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the prediction accuracy for classification by comparing the results into class with `rain_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = to_class(y_pred)\n",
    "\n",
    "classification_metric(Y_test_class,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.2. With `rain_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regLinLog = LinearRegression().fit(X_train, Y_train_log)\n",
    "\n",
    "prevLog = regLinLog.predict(X_test)\n",
    "prev = np.exp(prevLog) - 1\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `rain_log`, the value of MSE is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Penalized regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.1. Penalisation Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1.1. With `rain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the Lasso regression with the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regLasso = linear_model.Lasso()\n",
    "regLasso.fit(X_train, Y_train)\n",
    "prev = regLasso.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalisation parameter is optimized by the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = [{\"alpha\":np.linspace(0, 2, 200)}]\n",
    "regLasso = GridSearchCV(linear_model.Lasso(), param_grid = param, scoring = \"r2\", cv = 5,n_jobs=-1)\n",
    "regLassOpt = regLasso.fit(X_train, Y_train)\n",
    "# Optimal parameter\n",
    "regLassOpt.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regLassOpt.best_score_, regLassOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some previsions with the optimized value of `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = regLassOpt.predict(X_test)\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prev, Y_test,\"o\")\n",
    "plt.xlabel(\"Rain predicted\")\n",
    "plt.ylabel(\"Rain observed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prev,Y_test - prev,\"o\")\n",
    "plt.xlabel(u\"Predicted\")\n",
    "plt.ylabel(u\"Residus\")\n",
    "plt.hlines(0,-1,9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regLasso=linear_model.Lasso(alpha=regLassOpt.best_params_['alpha'])\n",
    "model_lasso=regLasso.fit(X_train,Y_train)\n",
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso retain \" + str(sum(coef != 0)) + \n",
    "      \" variables and delete \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(u\"Lasso model coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV(cv=5, alphas=np.array(range(1,200,1))/200.,n_jobs=-1,random_state=42).fit(X_train,Y_train)\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "# ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Mean of MSE', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: optimised by VC')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE of each validation: coordinate descent ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "from sklearn.linear_model import lasso_path\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train,Y_train, alphas=np.array(range(1,50,1))/20.,)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "for coef_l, s in zip(coefs_lasso, styles):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, linestyle=s,c='b')\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1.2. With `rain_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"alpha\": np.linspace(0, 2 , 200) }]\n",
    "\n",
    "regLasso = GridSearchCV(linear_model.Lasso(), param_grid = param, scoring = \"r2\",cv=5,n_jobs=-1)\n",
    "regLassOpt=regLasso.fit(X_train, Y_train_log)\n",
    "# optimal parameter \n",
    "regLassOpt.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regLassOpt.best_score_,regLassOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevLog = regLassOpt.predict(X_test)\n",
    "prev = np.exp(prevLog) -1\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.2. Penalisation Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the Ridge regression with the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "regRidge = Ridge()\n",
    "regRidge.fit(X_train, Y_train)\n",
    "prev = regRidge.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalisation parameter is optimized by the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"alpha\" : np.linspace(0, 1, 200)}]\n",
    "regRidge = GridSearchCV(Ridge(),param_grid = param, scoring = \"r2\",cv=10,n_jobs=-1)\n",
    "regRidOpt=regRidge.fit(X_train, Y_train)\n",
    "# optimal parameter\n",
    "regRidOpt.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regRidOpt.best_score_,regRidOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some previsions with the optimized value of `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = regRidOpt.predict(X_test)\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regRidge=linear_model.Ridge(alpha=regRidOpt.best_params_['alpha'])\n",
    "model_ridge=regRidge.fit(X_train,Y_train)\n",
    "model_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_ridge.coef_, index = X_train.columns)\n",
    "print(\"Ridge retains \" + str(sum(coef != 0)) + \n",
    "      \" variables and deletes \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(u\"Ridge model coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `rain_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"alpha\":np.linspace(0,100,200)}]\n",
    "regRidge = GridSearchCV(Ridge(), param,cv=5,n_jobs=-1)\n",
    "regRidOpt=regRidge.fit(X_train, Yr_log_train)\n",
    "# optimal parameter\n",
    "regRidOpt.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regRidOpt.best_score_,regRidOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevLog=regRidOpt.predict(X_test)\n",
    "prev = np.exp(prevLog) -1\n",
    "print(\"MSE=\",mean_squared_error(prev,Yr_test))\n",
    "print(\"R2=\",r2_score(Yr_test,prev))\n",
    "\n",
    "prev_class = to_class(prev)\n",
    "print(\"Accuracy score =\", accuracy_score(Yb_test.to_list(), prev_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regRidge=linear_model.Ridge(alpha=regRidOpt.best_params_['alpha'])\n",
    "model_ridge=regRidge.fit(X_train,Yr_train)\n",
    "model_ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.3. Penalisation Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "regElastic = ElasticNet()\n",
    "regElastic.fit(X_train,Y_train)\n",
    "prev = regElastic.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param=[{\"alpha\":np.linspace(0,2,200)}]\n",
    "regElastic = GridSearchCV(ElasticNet(), param, cv=5,n_jobs=-1)\n",
    "regElasOpt=regElastic.fit(X_train, Y_train)\n",
    "# optimal parameter\n",
    "regElasOpt.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regElasOpt.best_score_, regElasOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some previsions with the optimized value of `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev=regElasOpt.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regElastic=linear_model.ElasticNet(alpha=regElasOpt.best_params_['alpha'])\n",
    "model_elastic=regElastic.fit(X_train,Y_train)\n",
    "model_elastic.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_elastic.coef_, index = X_train.columns)\n",
    "print(\"Elastic retains \" + str(sum(coef != 0)) + \n",
    "      \" variables and deletes \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(u\"Elastic model coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regRidge=linear_model.Ridge(alpha=regRidOpt.best_params_['alpha'])\n",
    "model_ridge=regRidge.fit(X_train,Yr_train)\n",
    "model_ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3. Generalized Linear Models (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.1. Poisson Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `rain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "regPoisson = PoissonRegressor(alpha = 0, max_iter = 300)\n",
    "regPoisson.fit(X_train, Y_train)\n",
    "prev = regPoisson.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.2. Poisson Regression with penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = [{\"alpha\":np.linspace(0,2,200)}]\n",
    "regPoisson_l2 = GridSearchCV(PoissonRegressor(), param,cv=5,n_jobs=-1)\n",
    "regPoiOpt_l2 = regPoisson_l2.fit(X_train, Y_train)\n",
    "# optimal parameter\n",
    "regPoiOpt_l2.best_params_[\"alpha\"]\n",
    "print(\"Best R2 = %f, Best parameter = %s\" % (regPoiOpt_l2.best_score_,regPoiOpt_l2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = regPoiOpt_l2.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(np.array(Y_test_class), to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.3. Gamma Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import GammaRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"alpha\" : np.linspace(0.1, 1, 15)}\n",
    "gamma_reg_cv = GridSearchCV(GammaRegressor(max_iter=300), param_grid = param, cv = 10, n_jobs = -1)\n",
    "gamma_reg = gamma_reg_cv.fit(X_train, Y_train + 1e-5)\n",
    "\n",
    "print(\"The best parameter is \", gamma_reg_cv.best_params_)\n",
    "\n",
    "y_pred = gamma_reg.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, y_pred)\n",
    "classification_metric(Y_test_class, to_class(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.4. Tweedie Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"power\" : [0, 1, 1.5, 1.6, 1.7, 1.8, 2, 2.5, 3, 4], \"alpha\" : np.linspace(0.1, 1, 10)}\n",
    "tw_cv = GridSearchCV(TweedieRegressor(link = \"log\", max_iter = 300), param_grid= param, cv = 10, n_jobs = -1, scoring = \"neg_mean_absolute_error\")\n",
    "tw = tw_cv.fit(X_train, Y_train)\n",
    "print(\"The best parameter is \", tw_cv.best_params_)\n",
    "print(\"The best score is \", tw_cv.best_score_ )\n",
    "y_pred = tw.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, y_pred)\n",
    "classification_metric(Y_test_class, to_class(y_pred))\n",
    "classification_metric(Y_test_class, to_class(y_pred, eps = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.4. SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\"C\": np.linspace(0.01, 1, 200) }]\n",
    "svr = GridSearchCV(LinearSVR(), param, cv=10, n_jobs = -1, scoring = \"neg_mean_absolute_error\")\n",
    "svrLinOpt = svr.fit(X_train, Y_train)\n",
    "print(\"Best Mean cross-validated accuracy = %f, Best parameter = %s\" % (svrLinOpt.best_score_, svrLinOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = svrLinOpt.predict(X_test)\n",
    "\n",
    "regression_metric(Y_test, prev)\n",
    "classification_metric(Y_test_class, to_class(prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.5. Comparison of the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_reg = pd.read_excel(\"./Performance_Regression.xlsx\", header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_reg = perf_reg.drop(11)\n",
    "perf_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.1. Regression Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "x = np.arange(13)\n",
    "\n",
    "plt.plot(x, perf_reg[\"MSE\"], \"o\", label = \"MSE\")\n",
    "plt.plot(x, perf_reg[\"MAE\"], \"o\", label = \"MAE\")\n",
    "plt.plot(x, np.log(perf_reg[\"MAPE\"]), \"o\", label = \"log(MAPE)\")\n",
    "plt.plot(x, perf_reg[\"R2_Score\"], \"o\", label = \"R2 Score\")\n",
    "\n",
    "arg_min_mse, min_mse = np.argmin(perf_reg[\"MSE\"]), np.min(perf_reg[\"MSE\"])\n",
    "\n",
    "plt.annotate(s = \"Min MSE\", xy = [arg_min_mse, min_mse + 0.25], \n",
    "        xytext = [arg_min_mse, min_mse + 3.75],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_min_mae, min_mae = np.argmin(perf_reg[\"MAE\"]), np.min(perf_reg[\"MAE\"])\n",
    "\n",
    "plt.annotate(s = \"Min MAE\", xy = [arg_min_mae, min_mae + 0.25], \n",
    "        xytext = [arg_min_mae, min_mae + 3.75],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_min_mape, min_mape = np.argmin(perf_reg[\"MAPE\"]), np.min(np.log(perf_reg[\"MAPE\"]))\n",
    "\n",
    "plt.annotate(s = \"Min MAPE\", xy = [arg_min_mape, min_mape + 0.25], \n",
    "        xytext = [arg_min_mape, min_mape + 3.75],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_min_r2, min_r2 = np.argmax(perf_reg[\"R2_Score\"]), np.max(perf_reg[\"R2_Score\"])\n",
    "\n",
    "plt.annotate(s = \"Max r2\", xy = [arg_min_r2, min_r2 + 0.25], \n",
    "        xytext = [arg_min_r2, min_r2 + 3.75],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "plt.xticks(x, list(perf_reg[\"Model\"]), rotation ='vertical', fontsize = 16)\n",
    "plt.legend(loc = 3, fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Images/Reg_Performance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.2. Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_class = pd.read_excel(\"./Performance_Regression.xlsx\", header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "x = np.arange(14)\n",
    "\n",
    "plt.plot(x, perf_class[\"Accuracy\"], \"-o\", label = \"Accuracy\")\n",
    "plt.plot(x, perf_class[\"Avg_precision\"], \"-o\", label = \"Precision\")\n",
    "plt.plot(x, perf_class[\"Avg_recall\"], \"-o\", label = \"Recall\")\n",
    "plt.plot(x, perf_class[\"Avg_f1_score\"], \"-o\", label = \"F1 score\")\n",
    "\n",
    "arg_max_Accuracy, max_Accuracy = np.argmax(perf_class[\"Accuracy\"]), np.max(perf_class[\"Accuracy\"])\n",
    "\n",
    "plt.annotate(s = \"Max Accuracy\", xy = [arg_max_Accuracy, max_Accuracy ], \n",
    "        xytext = [arg_max_Accuracy - 4, max_Accuracy + 0.1],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_max_Avg_precision, max_Avg_precision = np.argmax(perf_class[\"Avg_precision\"]), np.max(perf_class[\"Avg_precision\"])\n",
    "\n",
    "plt.annotate(s = \"Max precision\", xy = [arg_max_Avg_precision, max_Avg_precision], \n",
    "        xytext = [arg_max_Avg_precision + 2, max_Avg_precision + 0.1],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_max_Avg_recall, max_Avg_recall = np.argmax(perf_class[\"Avg_recall\"]), np.max(perf_class[\"Avg_recall\"])\n",
    "\n",
    "plt.annotate(s = \"Max recall\", xy = [arg_max_Avg_recall, max_Avg_recall], \n",
    "        xytext = [arg_max_Avg_recall + 2, max_Avg_recall + 0.1],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "arg_max_f1, max_f1 = np.argmax(perf_class[\"Avg_f1_score\"]), np.max(perf_class[\"Avg_f1_score\"])\n",
    "\n",
    "plt.annotate(s = \"Max f1-score\", xy = [arg_max_f1, max_f1], \n",
    "        xytext = [arg_max_f1 + 3, max_f1 + 0.15],\n",
    "        arrowprops = dict(facecolor ='red', shrink = 0.001), fontsize = 14 )\n",
    "\n",
    "plt.xticks(x, list(perf_class[\"Model\"]), rotation ='vertical', fontsize = 16)\n",
    "plt.legend(loc = 3, fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Images/Reg_Class_Performance.pdf\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f155fbeb9494e5ce992090b8427abe3542dae7719d8ea0d05cb0b78608edd18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
